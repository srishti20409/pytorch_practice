{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/06/9x6v8vd55lv98_m_jp45b7f80000gn/T/ipykernel_53035/3298120882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='datasets/mnist/',transform=ToTensor(),download=True)\n",
    "test_dataset  = MNIST(root='datasets/mnist/',train=False,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader  = DataLoader(dataset=train_dataset, batch_size=batch_size*2, shuffle=False,pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2 #2-layer stacked LSTM\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #batch_first for shape (batch,seq_no,features)\n",
    "        self.fc = nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self,X_batch):\n",
    "        hidden_0 = torch.zeros(self.num_layers, X_batch.size(0), self.hidden_size).to(device)\n",
    "        cellstate_0 = torch.zeros(self.num_layers, X_batch.size(0),self.hidden_size).to(device)\n",
    "        \n",
    "        output,_ = self.lstm(X_batch, (hidden_0,cellstate_0))\n",
    "        #output shape is (batch_size,seq_no,hidden_size)\n",
    "        # we need hidden_state of the last element in the sequence for each of the batch inputs\n",
    "        output = self.fc(output[:,-1,:])\n",
    "        return output\n",
    "    \n",
    "    def train_step(self,X_batch):\n",
    "        images,labels = X_batch\n",
    "        images = images.reshape(-1,sequence_length,input_size)\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output,labels)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self,X_batch):\n",
    "        images,labels = X_batch\n",
    "        images = images.reshape(-1,sequence_length,input_size)\n",
    "        output = model(images)\n",
    "        loss = F.cross_entropy(output,labels)\n",
    "        _,preds = torch.max(output,dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
    "        return {'loss':loss,'acc':acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data,device):\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    else:\n",
    "        return data.to(device,non_blocking=True)\n",
    "    \n",
    "class DeviceDataLoader():\n",
    "    #Wrap dataloader to move data to device\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch,self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(train_loader,device)\n",
    "test_loader = DeviceDataLoader(test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,test_loader):\n",
    "    outputs = [model.test_step(batch) for batch in test_loader]\n",
    "    batch_losses = [x['loss'] for x in outputs]\n",
    "    batch_accs = [x['acc'] for x in outputs]\n",
    "    loss = torch.stack(batch_losses).mean()\n",
    "    acc = torch.stack(batch_accs).mean()\n",
    "    print(f\"loss is {loss} and acc is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 2.303302049636841 and acc is 0.10441667586565018\n"
     ]
    }
   ],
   "source": [
    "evaluate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader,num_epochs=2,lr=0.01,optim_func=torch.optim.Adam):\n",
    "    optim = optim_func(model.parameters(),lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            loss = model.train_step(batch)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        evaluate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.12334827333688736 and acc is 0.9633000493049622\n",
      "loss is 0.08870931714773178 and acc is 0.974266767501831\n"
     ]
    }
   ],
   "source": [
    "train(model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
